---
title: "Assessing Machine Translation Performance in Processing Ellipses"
author: "Isabelle Chang"
institute: "Rutgers University"
date: "2023/05/01 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default,hygge,rutgers,rutgers-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```
```{r load_libs, include=FALSE}
library("papaja")
library("tidyverse")
library("here")
library("ds4ling")
library("lmtest")
```

```{r load_data}

```
# Background Information
## Literature
- Machine translation errors often occur at the ellipsis site
- Accuracy of machine translations are correlated with ellipsis strategies
---
# Background Information
## Ellipsis types
- Verb Phrase Ellipsis (VPE, English only)
- Noun Phrase Ellipsis (NPE, English only)
- Sluicing
- Argument Ellipsis (Japanese only)
- VPE-Like Constructions with Verb Stranding (Japanese only)

---
# Methodology
## Experimental Setup
- 2 native English speakers and 2 Japanese speakers (1 native, 1 advanced L2) generated sentences
- 5 sentences per ellipsis type per source language (30 sentences total)
- Each sentence was run through the following machine translation services: Google Translate, Microsoft Bing, and DeepL
    - 45 translations per language, 90 total
---
# Methodology
## Translation Scoring - Alphabetical
```{r, alpha_scoring, echo=FALSE}
knitr::include_graphics(here("figs","alphabetical_scoring.png"))
```
---
# Methodology
## Translation Scoring - Numerical
```{r, num_scoring, echo=FALSE}
alphabetical_score <- c('A', 'B', 'C', 'D', 'E', 'F', 'G')
meaning_score <- c(1, 2, 3, 4, 5, 6, 7)
grammaticality_score <- c(1, 2, 3, 6, 7, 5, 4)
table <- data.frame(alphabetical_score,meaning_score,grammaticality_score)
table %>% knitr::kable()
```
---
# Results
## Descriptive Statistics
```{r, read_tidy, include=FALSE}
data_raw = read_csv(file=here("data","data_raw","497_raw.csv"))
data_raw %>%
  mutate(
    meaning = as.factor(`Meaning Score`),
    grammaticality = as.factor(`Grammaticality Score`)) %>%
  select(meaning, grammaticality, `Ellipsis type`) %>%
  pivot_longer(cols=c("meaning", "grammaticality"),
               names_to="metric",
               values_to = "num_score") %>%
  subset(num_score != "NA") %>%
  mutate(num_score = as.numeric(num_score)) %>%
  write_csv(here("data", "data_tidy", "497_tidy.csv"))

data_tidy = read_csv(file=here("data","data_tidy","497_tidy.csv"))
data_factor <- data_tidy %>%
  mutate(
    ellipsis_type = as.factor(`Ellipsis type`),
    metric_type = as.factor(`metric`)
  ) %>%
  select(ellipsis_type,metric_type,num_score)
data_factor
```

```{r desc_stats, echo=FALSE, message=FALSE}
ellipsis_stats <- data_factor %>%
  group_by(ellipsis_type,metric_type) %>%
  summarize(avg=mean(num_score)) %>%
  pivot_wider(names_from="metric_type", values_from="avg") %>%
  knitr::kable()

ellipsis_stats
```

- So far, these values suggest that grammaticality is prioritized over meaning in machine translation.
- Argument Ellipsis and VPE-like constructions scored the lowest in both grammaticality and meaning.
- Next steps:
    - Separating sluicing data by source language
    - Fitting classification models to explore other predictors (machine translation service, source language, etc...)
    - Accounting for variation in source language sentences